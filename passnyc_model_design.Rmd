---
title: "passnyc"
output:
  html_document:
    theme: lumen
    highlight: tango
    toc: true
    toc_float: true
    toc_depth: 1
    code_folding: hide
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(
  echo = T, warning = F, message = F,
  fig.width = 6,fig.asp = 0.618,out.width = "80%",fig.align = "center"
)
```

```{r load_libs}
library(tidyverse);library(plotly);library(leaflet);library(htmltools);library(magrittr)
```

# Getting data to inform us

Before we start, let's get the data for the analysis.  In addition to the official datasets, we also downloaded datasets using the Socrata API.  The datasets considered are shown in [Appendix A](#appendix_a) and the precise scripts used to access and clean the data are available in [Appendix B](#appendix_b).

```{r get_data}
schools <- 
  read_csv(
    file = "data/2016 School Explorer.csv",
    na = c("", "NA","N/A")
  )

shsat <- read_csv(file = "data/D5 SHSAT Registrations and Testers.csv")

# Clean names
names(schools) <- str_replace_all(names(schools), "\\(|\\)|\\?|\\-", "")
names(schools) <- tolower(str_replace_all(names(schools), " ", "_"))
names(shsat) <- tolower(str_replace_all(names(shsat), " |/", "_"))

# Get other Socrata datasets
nyc_attend <- read_csv("data/nyc_attend.csv")
nyc_school_crime <- read_csv("data/nyc_school_crime.csv")

```

## Cleaning the data

Then, we clean the data up to prepare it for joining with the other datasources.  

```{r clean_data}
schools <-
  schools %>%
  mutate(
    school_income_estimate = as.numeric(
      str_replace_all(school_income_estimate,"\\$|,","")
    ),
    sed_code = as.character(sed_code),
    economic_need_index = as.numeric(economic_need_index)
  ) %>%
  mutate_at(
    .vars = vars(matches("percent|%|rate")),
    .funs = funs(str_replace_all),
    pattern = "%", replacement = ""
  ) %>%
  mutate_at(
    .vars = vars(matches("percent|%|rate")),
    .funs = funs(as.numeric)
  ) %>%
  mutate(
    percent_nonwhite = 100 - percent_white
  )
  
```

We subset the SHSAT data provided in order to gain a single observation per school (i.e. `dbn`) for the most recent year available.

```{r clean_shsat}
shsat %<>%
  filter(grade_level <= 8) %>%
  group_by(dbn) %>%
  filter(
    year_of_shst == max(year_of_shst)
  ) %>%
  select(-school_name)

```

## Subsetting the Schools

The [Princeton Review](https://www.princetonreview.com/k12/shsat-information) notes that they "do not recommend waiting until 9th grade to take the test as these high schools have a very limited number of 10th grade seats available."  PASSNYC "partner[s] with NYC middle schools" to accomplish their objectives, and we thus filter the data to focus on these grade levels (i.e. grades 6-8).

```{r subset}
subset_schools <-
  schools %>%
  filter(as.numeric(grade_high) %in% 6:8) %>%
  filter(community_school == "Yes") %>%
  select(
    location_code, school_name, latitude, longitude, zip, grade_high, community_school, economic_need_index, 
    student_attendance_rate, percent_of_students_chronically_absent,school_income_estimate, `rigorous_instruction_%`:student_achievement_rating, percent_ell, percent_nonwhite,
    starts_with("grade_6"),starts_with("grade_7"),starts_with("grade_8")
  ) 
```

## Combining the data 

We then combine the various datasets referenced above, joining them at the level of the school, so that the dataset returns one row per school:

```{r subset_join}

schools_joined <- 
  subset_schools %>% 
  left_join(shsat, by = c("location_code" = "dbn")) %>%
  left_join(nyc_attend, by = "location_code") %>%
  left_join(nyc_school_crime, by = c("location_code" = "dbn")) %>%
  distinct(location_code, .keep_all = T)

```

# What is the problem?

Per [NY Times article](https://www.nytimes.com/2018/06/21/nyregion/what-is-the-shsat-exam-and-why-does-it-matter.html), "Beginning in the fall of 2019, the city would set aside 20 percent of seats for low-income students who score just below the lowest cutoff score."

# Who are we trying to help?

We want to identify schools with kids who are: 

- from backgrounds that would diversify the specialty schools (e.g. immigrants, kids with disabilities, poor students, hardships)
- eligible to take the SHSAT (*meaning that the school should meet the above criteria for the upper grades prior to high school, since these are the grades where kids take the SHSAT*)
- in schools that do not have significant resources but are performing relatively well anyway
- in schools that have a positive culture
- "stand to gain the most" from services like after school programs, test preparation, mentoring, or resources for parents
- low income, according to the 2019 admission policy

## Identifying relevant variables

### Related to student demographic characteristics:

There are `r ncol(schools)` variables in the schools dataset.  The variables below were identified as being most relevant to the questions at hand:

- Immigrants: `percent_ell`, `ends_with("_limited_english_proficient")`
- Economically disadvantaged: `economic_need_index`, `ends_with("_economically_disadvantaged")`. We also use the ``, which "reflects the socioeconomics of the school population."^[According to [the NYC Educator Guide](http://schools.nyc.gov/NR/rdonlyres/7B6EEB8B-D0E8-432B-9BF6-3E374958EA70/0/EducatorGuide_EMS_20131118.pdf)  It is calculated using the following formula: Economic Need Index = (Percent Temporary Housing) + (Percent 
HRA-eligible * 0.5) + (Percent Free Lunch Eligible * 0.5)]
- Disabilities: no variables were found in the schools data set, though we may be able to find other publically available data. 
- Absentees: `student_attendance_rate`, `percent_of_students_chronically_absent`
- Race/Ethnicity: While the PASSNYC problem statement indicated that more 'homogenous student body demographics' were a part of what it sought to address in developing specialized school enrollment for "a more diverse group of students", they did not explicitly identify which groups were over/under-represented.  We will keep these variables in to evaluate our model, but not use these variables to ilter schools out of the results.  Variables which identify race/ethnicity include: `percent_nonwhite`, `ends_with("_american_indian_or_alaska_native")`, `ends_with("_black_or_african_american")`, `ends_with("_hispanic_or_latino")`, `ends_with("_asian_or_pacific_islander")`, `ends_with("_white")`, `ends_with("_multiracial")`

We might want to look at schools which have a high percent of English language learners (ELL), which we can summarize by looking at the histogram below:

```{r}
schools_joined %>% plot_ly(x = ~percent_ell) %>% add_histogram()
```

However, the likelihood is that some schools with a high percent of ELL won't necessarily have a high percentage of African-American or Latino students.  So, we want to start by looking at all of the relevant variables in one view, called a heatmap:

```{r}
schools_joined %>% 
  set_rownames(.$location_code) %>%
  select(economic_need_index:percent_nonwhite) %>%
  select(-ends_with("_rating"),-school_income_estimate) %>%
  scale() %>%
  # complete.cases() %>%
  # mutate_at(
  #   .vars = vars(percent_of_students_chronically_absent,rigorous_instruction_%),
  #   .funs = funs(. * -1)
  # )
  d3heatmap::d3heatmap(
    colors = viridisLite::viridis(100, direction = -1),
    k_row = 8,
    na.rm = T,
    show_grid = F
  )

```

Based on exploration of these clusters, we realize that students in our target populations (i.e. those with a high number/percent of African American/Hispanic students) fall across various clustered groups when we consider all relevant variables.  Some are in schools with rigorous instruction, some in schools with weak community ties, etc.  So we we want to focus the issue by filtering out schools that do not meet critical values on the critical variables of: `percent_nonwhite`, `percent_ell`, and `economic_need_index`.  One we limit our view to schools that meet these criteria, we can use the other variables to identify the environments where PASSNYC's work could best be integrated into the environment.

```{r}
tst <-
  schools_joined %>%
  #group_by(location_code) %>%
  select(starts_with("grade_")) %>%
  select(contains("ela")) %>%
  select(-ends_with("_white"))
```


```{r}
schools_matrix <-
  schools_joined %>% 
  set_rownames(.$location_code) %>%
  select(economic_need_index:percent_nonwhite,starts_with("grade_")) %>%
  select(-ends_with("_rating"),-school_income_estimate,-grade_high,-grade_level) %>%
  filter(
    ntile(percent_nonwhite, 4) > 1
    & ntile(percent_ell, 4) > 1
    & ntile(economic_need_index, 4) > 1
  ) %>%
  # filter(complete.cases(.)) %>%
  scale() %>%
  # And remove columns with all NaN values
  .[,colSums(!is.nan(.))>0] %>%
  as.tibble() %>%
  mutate(
    not_chronic_absent = percent_of_students_chronically_absent * -1,
    need_instruction = `rigorous_instruction_%` * -1
  ) %>%
  select(-percent_of_students_chronically_absent,-`rigorous_instruction_%`) 

schools_matrix %>%
  # select(-starts_with("grade")) %>%
  d3heatmap::d3heatmap(
    colors = viridisLite::viridis(30, direction = -1),
    k_row = 8,
    na.rm = T,
    show_grid = F
  )

```


```{r}

library(broom); library(irlba)
# sparse_matrix <- subset_schools %>% tidytext::cast_sparse(User, Tag, Value)

schools_pca <- 
  schools_matrix %>%
  # Convert to matrix
  as.matrix() %>% 
  irlba::prcomp_irlba(n = ncol(schools_matrix) - 1)

tidied_pca <- 
  bind_cols(
    variable = colnames(schools_matrix),
    tidy(schools_pca$rotation)
  ) %>%
  gather(pc, contribution, -variable)

```


```{r}
  
  tidied_pca %>% 
    filter(pc %in% paste0("PC", 1:6)) %>%
    ggplot(aes(variable, contribution, fill = variable)) +
    geom_col(show.legend = FALSE, alpha = 0.8) +
    theme(
      axis.text.x = element_blank(), 
      axis.ticks.x = element_blank(),
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank()
    ) + 
    labs(x = "Variable",
         y = "Relative importance in each principal component") +
    facet_wrap(~ pc, ncol = 2)
```

```{r}

percent_variation <- schools_pca$sdev^2 / sum(schools_pca$sdev^2)

augmented_pca <- 
  bind_cols(
    row_n = rownames(schools_matrix),
    tidy(schools_pca$x)
  )

augmented_pca %>%
    mutate(row_n = as.integer(row_n)) %>%
    ggplot(aes(PC1, PC2)) +
    geom_point(size = 1.3, color = "midnightblue", alpha = 0.1) +
    labs(
      x = paste0("Principal component 1 (",  round(percent_variation[1] * 100, digits = 1), "%)"), 
      y = paste0("Principal component 2 (",  round(percent_variation[2] * 100, digits = 1),"%)"),
      title = "Projection of Schools on to the first two principal components",
      subtitle = "The high dimensional space can be projected onto components we have explored"
    )

```


# How are we trying to help them?

- Motivating them to study for the SHSAT
- Tutoring and test preparation for the SHSAT
- Application to specialty high schools

# Where is the best place to go to help them?

```{r map}

schools %>% 
  leaflet() %>% 
  addProviderTiles(providers$CartoDB.Positron) %>%
  addCircleMarkers(
    ~longitude, 
    ~latitude,
    popup = ~paste0(
      "<b>School Name:</b> ",htmlEscape(school_name),"<br/>",
      "<b>Address:</b> ",htmlEscape(`address_full`)
    ),
    stroke = FALSE,
    radius = 5,
    fillOpacity = 0.6
  )

```

- Where there are the greatest number of kids fitting this criteria
- With students in the most advanced grades who are candidates for specialty schools (*How long does it take to establish relationships with schools?  If awhile, may want to pick schools that meet criteria for multiple upper grades in a row*)
- Can kids come from other schools if they're nearby?
- PASSNYC may also choose the strategic approach of combining PASSNYC's resources with those of [community schools](https://www1.nyc.gov/site/communityschools/index.page) in order to maximize the likelihood that individuals from minority populations would successfully take and pass the SHSAT exam.



```{r}
schools %>% 
  plot_ly(
    x=~fct_reorder(school_name,percent_ell,.desc = T),
    y=~percent_ell
  )

```

```{r}
schools %>% 
  plot_ly(
    x=~percent_ell,
    y=~economic_need_index
  ) %>%
  add_markers()
```



# Maps/Location



# Appendices

## Appendix A: Open NYC Datasets {#appendix_a}

The following datasets were considered as candidates for inclusion in the analysis:

- [Universal Pre-K School Locations](https://www.kaggle.com/new-york-city/nyc-universal-pre-k-upk-school-locations)
- [School District Breakdowns](https://www.kaggle.com/new-york-city/nyc-school-district-breakdowns)
- [SAT Results](https://www.kaggle.com/new-york-city/new-york-city-sat-results)
- [Graduation Outcomes](https://www.kaggle.com/new-york-city/new-york-city-graduation-outcomes)
- [School Attendance and Enrollment](https://www.kaggle.com/new-york-city/ny-school-attendance-and-enrollment)
- [NYPL Branch Services](https://www.kaggle.com/new-york-city/nyc-nypl-branch-services)

## Appendix B: Socrata API Scripts {#appendix_b}

```{r get_api_data, eval=FALSE, include=TRUE}

library("RSocrata")

nyc_attend_12_17 <- read.socrata(
  url = "https://data.cityofnewyork.us/resource/s9bd-hden.json",
  app_token = Sys.getenv("socrata_token"),
  email     = Sys.getenv("socrata_email"),
  password  = Sys.getenv("socrata_pw")
)

nyc_attend_ytd <- read.socrata(
  url = "https://data.cityofnewyork.us/resource/5vg6-6vfs.json",
  app_token = Sys.getenv("socrata_token"),
  email     = Sys.getenv("socrata_email"),
  password  = Sys.getenv("socrata_pw")
)

nyc_school_crime <- read.socrata(
  url = "https://data.cityofnewyork.us/resource/sm8b-9vim.json",
  app_token = Sys.getenv("socrata_token"),
  email     = Sys.getenv("socrata_email"),
  password  = Sys.getenv("socrata_pw")
)

nyc_sat_2012 <- read.socrata(
  url = "https://data.cityofnewyork.us/resource/734v-jeq5.json",
  app_token = Sys.getenv("socrata_token"),
  email     = Sys.getenv("socrata_email"),
  password  = Sys.getenv("socrata_pw")
)

nyc_graduate <- read.socrata(
  url = "https://data.cityofnewyork.us/resource/ns8x-c6af.json",
  app_token = Sys.getenv("socrata_token"),
  email     = Sys.getenv("socrata_email"),
  password  = Sys.getenv("socrata_pw")
)

```

Clean the attendance dataset and summarize for only the grades in question:

```{r clean_attend, eval=FALSE, message=FALSE, warning=FALSE, include=TRUE, paged.print=FALSE}

nyc_attend_ytd %<>% 
  mutate(schoolyear = "2017 - 2018") %>%
  select(absent,calmonth,gradelevel,gradesort,monthcode,present,released,rostercount,school,schoolyear)

nyc_attend <-
  nyc_attend_12_17 %>%
  bind_rows(nyc_attend_ytd) %>%
  mutate_at(
    .vars = vars(absent,gradesort,present,released,rostercount),
    .funs = funs(as.numeric)
  ) %>%
  mutate(tot = present + absent) %>%
  select(school,schoolyear,calmonth,monthcode,gradelevel,gradesort,present,absent,released,rostercount,tot) %>%
  group_by(school,schoolyear,gradelevel) %>% 
  summarize(
    absent = sum(absent),
    present = sum(present),
    tot = sum(tot),
    n_months = n_distinct(calmonth)
  ) %>%
  mutate(
    gradelevel = recode(
      gradelevel,
      `6` = "06",
      `7` = "07",
      `8` = "08",
      `9` = "09"
    )
  ) %>%
  filter(
    gradelevel %in% c("06","07","08")
    & schoolyear %in% c("2015 - 2016","2016 - 2017","2017 - 2018")
  ) %>%
  group_by(school) %>%
  summarize(
    absent = sum(absent),
    present = sum(present),
    tot = sum(tot),
    n_months = sum(n_months)
  ) %>%
  mutate(
    avg_absent = round(absent / n_months, digits = 1),
    avg_present = round(present / n_months, digits = 1),
    avg_tot = round(tot / n_months, digits = 1),
    attend_rate = round(present / tot * 100, digits = 1)
  ) %>%
  select(location_code = school, avg_tot, attend_rate)

rm(nyc_attend_12_17); rm(nyc_attend_ytd)

write_csv(nyc_attend,"data/nyc_attend.csv")

```

Next we clean the crime dataset:

```{r clean_crime, eval=FALSE, message=FALSE, warning=FALSE, include=TRUE, paged.print=FALSE}
nyc_school_crime <-
  nyc_school_crime %>%
  mutate_at(
    .vars = vars(ends_with("_n")),
    .funs = funs(as.numeric)
  ) %>%
  select(dbn,school_year,ends_with("_n")) %>%
  select(-starts_with("avg")) %>%
  filter(is.na(major_n) == F) %>%
  group_by(dbn) %>%
  filter(school_year == max(school_year)) %>%
  rename(
    crime_yr = school_year,
    major_crimes = major_n,
    noncrim_crimes = nocrim_n,
    other_crimes = oth_n,
    property_crimes = prop_n,
    violent_crimes = vio_n
  )

write_csv(nyc_school_crime,"data/nyc_school_crime.csv")
```